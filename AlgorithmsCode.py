# -*- coding: utf-8 -*-
"""Final Project Code

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YLllWMQwHCqpg6cTZT3xzpgIBCW0Bwcj
"""

# Algorithms
from sklearn.neighbors import KNeighborsClassifier
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import SVC
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.naive_bayes import GaussianNB

# Performance
from sklearn.model_selection import train_test_split
import time
from sklearn.metrics import accuracy_score

def test_algorithms(classes, features, name_dataset):
    '''
    This function will take in data and run through three classification algorithms: 
    K-nearest neighbor, Gaussian Process Classifier, Linear Discriminant Analysis, and Support Vector Machine.
    It will return the time it took for the algorithm to run through the data,
    as well as the accuracy of the data. The data inputed is assumed to already have
    been processed for matrix operations.
    '''
    x_train, x_test, y_train, y_test = train_test_split(features, classes, random_state = 0)
    
    # Algorithm classifiers
    knn = KNeighborsClassifier()
    gpc = GaussianProcessClassifier()
    lda = LinearDiscriminantAnalysis()
    svm = SVC()
    dt = tree.DecisionTreeClassifier()
    rnf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)
    gradDescent = SGDClassifier(penalty=None)
    naiveBayes = GaussianNB()
    
    # K-nearest neighbor
    start_time_knn = time.time()

    knn.fit(x_train, y_train)
    knn_pred = knn.predict(x_test)
    knn_score =  accuracy_score(y_test, knn_pred)

    end_time_knn = time.time() - start_time_knn

    # Gaussian Process Classifier
    start_time_gpc = time.time()

    gpc.fit(x_train, y_train)
    gpc_pred = gpc.predict(x_test)
    gpc_score = accuracy_score(y_test, gpc_pred)

    end_time_gpc = time.time() - start_time_gpc

    # Linear-discriminant analysis
    start_time_lda = time.time()

    lda.fit(x_train, y_train)
    lda_pred = lda.predict(x_test)
    lda_score = accuracy_score(y_test, lda_pred)

    end_time_lda = time.time() - start_time_lda

    # Support Vector Machine
    start_time_svm = time.time()

    svm.fit(x_train, y_train)
    svm_pred = lda.predict(x_test)
    svm_score = accuracy_score(y_test, lda_pred)

    end_time_svm = time.time() - start_time_lda
    
    # Decision Tree
    start_time_dt = time.time()
    
    dt.fit(x_train, y_train)
    dt_score = dt.score(x_test, y_test)
    
    end_time_dt = time.time() - start_time_dt
    
    # Random Forest
    start_time_rnf = time.time()
    
    rnf.fit(x_train, y_train)
    rnf_score = rnf.score(x_test, y_test)
    
    end_time_rnf = time.time() - start_time_rnf
    
    # Gradient Descent
    start_time_gd = time.time()
    gradDescent.fit(x_train, y_train)
    gd_score = gradDescent.score(x_test, y_test)
    end_time_gd = time.time() - start_time_gd
    
    # Naive Bayes
    start_time_NB = time.time()
    naiveBayes.fit(x_train,y_train)
    NB_score = naiveBayes.score(x_test, y_test)
    end_time_NB = time.time() - start_time_NB
    
    return { 'Dataset': name_dataset,
    'KNN Accuracy': knn_score, 'KNN Time': end_time_knn, 
    'GPC Accuracy': gpc_score, 'GPC Time': end_time_gpc, 
    'LDA Accuracy': lda_score, 'LDA Time': end_time_lda, 
    'SVM Accuracy': svm_score, 'SVM Time': end_time_svm,
    'Decision Tree Accuracy': dt_score, 'DT Time': end_time_dt,
    'Random Forest Accuracy': rnf_score, 'RNF Time': end_time_rnf,
    'Gradient Descent Accuracy': gd_score, 'GD Time': end_time_gd,
    'Naive Bayes Accuracy':NB_score, 'NB Time': end_time_NB}

"""# **Mushroom Dataset**"""

import pandas as pd
mushrooms = pd.read_csv('mushrooms.csv')

from sklearn.preprocessing import LabelEncoder

# label encode mushroom data
labelencoder = LabelEncoder()
mushrooms = mushrooms.apply(labelencoder.fit_transform)

mushroom_features = mushrooms.iloc[:,1:]
mushroom_labels = mushrooms.iloc[:,0]

mushroom_test = test_algorithms(mushroom_labels, mushroom_features, "Mushroom")

mushroom_test

"""# **Wine Dataset**"""

# set features and labels wine data
Wine = pd.read_csv('winequality-red.csv')
wine_features = Wine.iloc[:,:-1]
wine_labels = Wine.iloc[:,-1]

wine_test = test_algorithms(wine_labels, wine_features, "Wine")

wine_test

"""# **Spam Dataset**"""

# set features and labels for spam data
spambase = pd.read_csv("spambase.data", header=None)
spam_features= spambase.iloc[:,:-1]
spam_labels = spambase.iloc[:,-1]

spam_test = test_algorithms(spam_labels, spam_features, "Spam")

spam_test

"""# **Police Data**"""

#Fixed with classification based on Race
from sklearn import preprocessing

start = time.time()

data = pd.read_csv('database.csv')

data = data.dropna()

classLabels = data['race']

features = pd.DataFrame(data, columns=data.columns[1:4], index=data.index)

features = features.join((pd.DataFrame(data["age"].astype(int))))

features = features.join(pd.DataFrame(data, columns=data.columns[6:6], index=data.index), sort = False)

features = features.join(pd.DataFrame(data, columns=data.columns[8:9], index=data.index), sort = False)

features = features.join((pd.DataFrame(data["signs_of_mental_illness"].astype(int))))

features = features.join(pd.DataFrame(data, columns=data.columns[11:len(data.columns)-1], index=data.index), sort = False)

  
encoder = preprocessing.LabelEncoder()
encoder.fit(classLabels)


encodedClassLabels = encoder.transform(classLabels)
encodedFeatures = features.apply(encoder.fit_transform)

police_test = test_algorithms(encodedClassLabels, encodedFeatures, "Police Data")

police_test

"""# **Faces Dataset**"""

!tar xzf crop_part1.tar.gz

import numpy as np
import cv2
import os
import sys
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow import keras


#Have to run twice if starting from fresh run so that 
#directory we change to the correct one

directory = os.getcwd()

if directory == '/content':
  os.chdir('crop_part1')

genderLabels = list()
genderTextLabels = list()
'''
gender labels is the list of validated genders for each corresponding
photo
'''

for filename in os.listdir(directory)[0:3000]:
  split = filename.split('_')
  genderLabels.append(split[1])
  #img = cv2.imread(filename,1)
  #images.append(img)

filelist = os.listdir(directory)[0:3000]

images = np.array([cv2.imread(fname, 1) for fname in filelist])

encoded = images
imageDict = list()

for image in encoded:
  imageList = list() 
  for pixel in image:
    pixel = np.mean(pixel)
    imageList.append(pixel)
  imageDict.append(imageList)

for gender in genderLabels:
  if gender == "0":
    genderTextLabels.append(0)
  
  elif gender == "1":
    genderTextLabels.append(1)
  else:
    genderTextLabels.append(1)

images = images / 255.0

X_train, X_test, y_train, y_test = train_test_split(images, genderTextLabels, test_size=0.30)

from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization


cnn = Sequential()
cnn.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(200,200,3)))
cnn.add(BatchNormalization())

cnn.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))
cnn.add(BatchNormalization())
cnn.add(MaxPooling2D(pool_size=(2, 2)))
cnn.add(Dropout(0.25))

cnn.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
cnn.add(BatchNormalization())
cnn.add(Dropout(0.25))

cnn.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
cnn.add(BatchNormalization())
cnn.add(MaxPooling2D(pool_size=(2, 2)))
cnn.add(Dropout(0.25))

cnn.add(Flatten())

cnn.add(Dense(512, activation='relu'))
cnn.add(BatchNormalization())
cnn.add(Dropout(0.5))

cnn.add(Dense(128, activation='relu'))
cnn.add(BatchNormalization())
cnn.add(Dropout(0.5))

cnn.add(Dense(10, activation='softmax'))

cnn.compile(optimizer='adam', 
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

cnn_images_start_time = time.time()

cnn.fit(X_train, y_train, epochs=10)

cnn_images_end_time = time.time() - cnn_images_start_time

faces_test

face_test = test_algorithms(genderTextLabels, imageDict, "Female/Male Photos")

face_test

os.chdir("/content")

cnn_accuracy_photos = 0.8418
print(cnn_images_end_time)

"""# **Neural Network Code**"""

from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Activation
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization

def CNN(X,Y, test_images, test_labels,dim):
  model = Sequential()

  model.add(Dense(11, input_shape= dim ))
  model.add(Activation('relu'))

  model.add(Dense(5))
  model.add(Activation('relu'))

  model.add(Dense(2))
  model.add(Activation('softmax'))

  
  model.compile(optimizer='adam', 
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])
  
  hist = model.fit(X, Y,
         nb_epoch=50,
         shuffle=True,
         batch_size=128,
         validation_data=(X, Y))
  
  test_loss, test_acc = model.evaluate(test_images, test_labels)

  return(test_acc)

import pandas as pd
data = pd.read_csv("mushrooms.csv")
features = pd.DataFrame(data, columns=data.columns[1:len(data.columns)], index=data.index)
classLabels = data['class']

labelencoder = LabelEncoder()
mushrooms = mushrooms.apply(labelencoder.fit_transform)

mushroom_features = mushrooms.iloc[:,1:]
mushroom_labels = mushrooms.iloc[:,0]

#encoder = preprocessing.LabelEncoder()
#encoder.fit(classLabels)

#encodedClassLabels = encoder.transform(classLabels)
#encodedFeatures = features.apply(encoder.fit_transform)

""" 
Split data into train and test sets using 30% test set size.
"""

x_train, x_test, y_train, y_test = train_test_split(mushroom_features, 
                                                    mushroom_labels, test_size = 0.3)
cnn_mush_starttime = time.time()
cnn_accuracy_mush = CNN(x_train, y_train, x_test, y_test, [22])
cnn_mush_end_time = time.time() - cnn_mush_starttime

import numpy as np
import cv2
import os
import sys
import pandas
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow import keras

data = pandas.read_csv('winequality-red.csv')
features = pandas.DataFrame(data, columns=data.columns[1:len(data.columns)], index=data.index)
classLabels = data['quality']

#print(data.shape)

encoder = preprocessing.LabelEncoder()
encoder.fit(classLabels)

encodedClassLabels = encoder.transform(classLabels)
encodedFeatures = features.apply(encoder.fit_transform)

encodedClassLabels = np.true_divide(encodedClassLabels, 5)

#print(type(encodedClassLabels))
""" 
Split data into train and test sets using 30% test set size.
"""

x_train, x_test, y_train, y_test = train_test_split(encodedFeatures, 
                                                    encodedClassLabels, test_size = 0.3)
cnn_wine_start = time.time()
cnn_wine_accuracy = CNN(x_train, y_train, x_test, y_test, [11])
cnn_wine_end = time.time() - cnn_wine_start

nums = list(range(0,57))
nums.append("Class")
data = pd.read_csv('spambase.data', names = nums)

features = pd.DataFrame(data, columns=data.columns[0:len(data.columns)-1], index=data.index)
classLabels = data["Class"]


encoder = preprocessing.LabelEncoder()
encoder.fit(classLabels)

encodedClassLabels = encoder.transform(classLabels)
encodedFeatures = features.apply(encoder.fit_transform)

x_train, x_test, y_train, y_test = train_test_split(encodedFeatures, 
                                                    encodedClassLabels, test_size = 0.3)
cnn_spam_start = time.time()
cnn_accuracy_spam = CNN(x_train, y_train, x_test, y_test, [57])
cnn_spam_end = time.time()

import pandas as pd

data = pd.read_csv('database.csv')

data = data.dropna()

classLabels = data['race']

features = pd.DataFrame(data, columns=data.columns[1:4], index=data.index)

features = features.join((pd.DataFrame(data["age"].astype(int))))

features = features.join(pd.DataFrame(data, columns=data.columns[6:6], index=data.index), sort = False)

features = features.join(pd.DataFrame(data, columns=data.columns[8:9], index=data.index), sort = False)

features = features.join((pd.DataFrame(data["signs_of_mental_illness"].astype(int))))

features = features.join(pd.DataFrame(data, columns=data.columns[11:len(data.columns)-1], index=data.index), sort = False)

  
encoder = preprocessing.LabelEncoder()
encoder.fit(classLabels)


encodedClassLabels = encoder.transform(classLabels)
encodedFeatures = features.apply(encoder.fit_transform)

encodedClassLabels = np.true_divide(encodedClassLabels, 5)


x_train, x_test, y_train, y_test = train_test_split(encodedFeatures, 
                                                    encodedClassLabels, test_size = 0.3)
cnn_start_police = time.time()
cnn_accuracy_police = CNN(x_train, y_train, x_test, y_test, [8])
cnn_end_police = time.time() - cnn_start_police

cnn_dict_accuracy = {'Photos': cnn_accuracy_photos, 'Wine': cnn_wine_accuracy, 'Police': cnn_accuracy_police,  'Mushroom': cnn_accuracy_mush,  'Spam': cnn_accuracy_spam }


cnn_dict_time = {'Photos': cnn_images_end_time , 'Wine': cnn_wine_end, 'Police': cnn_end_police, 'Mushroom': cnn_mush_end_time,  'Spam': cnn_spam_end}


cnn_dicts = [cnn_dict_accuracy, cnn_dict_time]
cnn_df = pd.concat([pd.Series(d) for d in cnn_dicts], axis=1)

cnn_df.columns = ['Accuracy', 'Time']
cnn_df

"""# **Plots**"""

mydicts = [faces_test, wine_test, police_test, mushroom_test, spam_test]
df = pd.concat([pd.Series(d) for d in mydicts], axis=1).T
df = df.set_index("Dataset")
df

import seaborn as sns

plt.rcParams["figure.figsize"] = (14,6)

fig, ax =plt.subplots(1,2)
sns.barplot(x=df.index, y=df['KNN Accuracy'], ax = ax[0]).set_title('KNN Accuracy')
sns.barplot(x=df.index, y=df['KNN Time'], ax = ax[1]).set_title('KNN Time')

fig, ax =plt.subplots(1,2)
sns.barplot(x=df.index, y=df['GPC Accuracy'], ax = ax[0]).set_title('GPC Accuracy')
sns.barplot(x=df.index, y=df['GPC Time'], ax = ax[1]).set_title('GPC Time')

fig, ax =plt.subplots(1,2)
sns.barplot(x=df.index, y=df['LDA Accuracy'], ax = ax[0]).set_title('LDA Accuracy')
sns.barplot(x=df.index, y=df['LDA Time'], ax = ax[1]).set_title('LDA Time')

fig, ax =plt.subplots(1,2)
sns.barplot(x=df.index, y=df['SVM Accuracy'], ax = ax[0]).set_title("SVM Accuracy")
sns.barplot(x=df.index, y=df['SVM Time'], ax = ax[1]).set_title('SVM Time')

fig, ax =plt.subplots(1,2)
sns.barplot(x=df.index, y=df['Decision Tree Accuracy'], ax = ax[0]).set_title('DT Accuracy')
sns.barplot(x=df.index, y=df['DT Time'], ax = ax[1]).set_title('DT Time')

fig, ax =plt.subplots(1,2)
sns.barplot(x=df.index, y=df['Random Forest Accuracy'], ax = ax[0]).set_title('RNF Accuracy')
sns.barplot(x=df.index, y=df['RNF Time'], ax = ax[1]).set_title('RNF Time')

fig, ax =plt.subplots(1,2)
sns.barplot(x=df.index, y=df['Gradient Descent Accuracy'], ax = ax[0]).set_title('GD Accuracy')
sns.barplot(x=df.index, y=df['GD Time'], ax = ax[1]).set_title('GD Time')

fig, ax =plt.subplots(1,2)
sns.barplot(x=df.index, y=df['Naive Bayes Accuracy'], ax = ax[0]).set_title('NB Accuracy')
sns.barplot(x=df.index, y=df['NB Time'], ax = ax[1]).set_title('NB Time')

fig, ax =plt.subplots(1,2)
plt.rcParams["figure.figsize"] = (14,6)
sns.barplot(x=cnn_df.index, y=cnn_df['Accuracy'], ax = ax[0]).set_title('CNN Accuracy')
sns.barplot(x=cnn_df.index, y=np.log(cnn_df['Time']), ax = ax[1]).set_title('CNN Time')

plt.xticks(rotation=90)

